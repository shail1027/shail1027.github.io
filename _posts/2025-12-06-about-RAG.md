---
title: RAG란?
description:
author: Lee Yebin
date: 2025-12-06 11:33:00 +0800
categories: [연구실, 이론 정리]
tags: [RAG, LLM]
pin: false
math: true
mermaid: true
# image:
#   path: 
#   lqip: 
#   alt:
---

# RAG (Retrieval-Augmented Generation)

## 1. 개요 (Overview)

**RAG(Retrieval-Augmented Generation, 검색 증강 생성)** 는 대규모 언어 모델(LLM)의 출력을 최적화하기 위한 기술 프레임워크이다. 이는 모델이 학습한 내부 파라미터 지식(Parametric Knowledge)에만 의존하지 않고, **신뢰할 수 있는 외부 지식 베이스(External Knowledge Base)로부터 관련 정보를 검색(Retrieval)** 하여 답변 생성(Generation) 시 참고하도록 만드는 방식이다.

## 2. 도입 배경 및 필요성 (Necessity)

LLM(GPT, Llama, Qwen 등)은 강력한 언어 능력을 보유하고 있으나, 의료 및 전문 비즈니스 영역에 단독으로 적용하기에는 다음과 같은 치명적인 한계가 존재한다. RAG는 이러한 문제를 구조적으로 해결한다.

| 한계점 (Pain Point)         | 상세 설명                                                                                | RAG의 해결 방안                                                                                                 |
| :-------------------------- | :--------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------- |
| **환각 (Hallucination)**    | 모델이 사실이 아닌 정보를 그럴듯하게 지어내는 현상이다. 특히 의료 분야에서는 치명적이다. | 답변의 근거가 되는 **실제 문서(Fact)** 를 프롬프트에 제공하여, 모델이 주어진 문맥 내에서만 답하도록 제어한다.   |
| **지식의 최신성 부재**      | 모델은 학습 시점(Cut-off date) 이후의 최신 정보를 알지 못한다.                           | 새로운 진료 지침이나 논문을 벡터 DB에 업데이트하기만 하면 **재학습 없이** 즉시 반영할 수 있다.                  |
| **폐쇄적 데이터 접근 불가** | 병원 내부 데이터(EMR) 등 비공개 데이터는 범용 모델에 학습되어 있지 않다.                 | 병원 내부 문서를 보안이 유지된 **로컬 벡터 DB**에 연동하여, 외부 유출 없이 내부 데이터를 활용해 답변할 수 있다. |
| **설명 불가능성**           | 답변의 출처나 근거를 명확히 제시하기 어렵다.                                             | 답변 생성 시 **"참고 문헌(Reference Source)"** 과 해당 페이지를 함께 제시하여 신뢰도를 확보한다.                |


## 3. RAG 아키텍처 및 동작 프로세스 (Workflow)

RAG 시스템은 크게 데이터를 검색 가능한 형태로 만드는 **(1) 데이터 구축(Indexing)** 단계와, 사용자의 질문에 답하는 **(2) 서비스(Retrieval & Generation)** 단계로 구분된다.




### Phase 1: 데이터 구축 파이프라인 (Indexing Pipeline)
MIMIC 데이터와 같은 비정형 텍스트를 LLM이 이해하고 검색할 수 있는 형태로 변환하는 과정이다.

1.  **데이터 로드 (Document Loading):** PDF, CSV, TXT, DB 등 다양한 소스에서 원시 데이터를 추출한다.
2.  **청킹 (Chunking):** 긴 문서를 모델의 컨텍스트 윈도우(Context Window)에 맞게, 그리고 검색의 정확도를 높이기 위해 의미 단위의 작은 조각(Chunk)으로 분할한다.
    * *전략:* Recursive Character Splitting(문단/문장 단위 재귀적 분할), Semantic Chunking(의미적 유사도 기반 분할) 등을 사용한다.
3.  **임베딩 (Embedding):** 텍스트 조각을 컴퓨터가 의미를 계산할 수 있도록 고차원의 숫자 벡터(Vector)로 변환한다.
4.  **벡터 저장소 적재 (Vector Store Indexing):** 생성된 벡터와 원본 텍스트, 메타데이터(환자 ID, 날짜 등)를 벡터 데이터베이스(Vector DB)에 저장한다.

### Phase 2: 검색 및 생성 파이프라인 (Retrieval & Generation Pipeline)
실제 사용자가 시스템을 사용할 때 실시간으로 일어나는 과정이다.

1.  **질문 변환 (Query Embedding):** 사용자의 질문(예: "환자의 주요 기저질환은?")을 문서와 동일한 임베딩 모델을 사용해 벡터로 변환한다.
2.  **유사도 검색 (Semantic Search):** 벡터 DB 공간상에서 질문 벡터와 거리가 가장 가까운(의미적으로 유사한) 문서 조각 상위 k개(Top-k)를 검색한다.
    * *알고리즘:* 코사인 유사도(Cosine Similarity)가 가장 보편적으로 사용된다.
3.  **프롬프트 증강 (Prompt Augmentation):** 검색된 문서 조각(Context)과 사용자의 질문(Query)을 결합하여, LLM에게 전달할 최종 프롬프트를 구성한다.
    * *Prompt Template 예시:*
        ```text
        당신은 유능한 전문의다. 아래 [관련 의무기록]만을 근거로 사용하여 [질문]에 대해 답변하라. 만약 정보가 없다면 모른다고 답하라.
        
        [관련 의무기록]:
        ... (검색된 MIMIC 환자 기록 청크 1) ...
        ... (검색된 MIMIC 환자 기록 청크 2) ...
        
        [질문]: 이 환자가 입원 기간 동안 겪은 급성 신손상(AKI)의 원인은 무엇인가?
        ```
4.  **답변 생성 (Generation):** 완성된 프롬프트를 LLM에 입력하여 최종 답변을 생성하고 사용자에게 전달한다.

---

## 4. RAG vs. Fine-tuning 비교


| 비교 항목         | Fine-tuning (학습)                                              | RAG (검색)                                                                  |
| :---------------- | :-------------------------------------------------------------- | :-------------------------------------------------------------------------- |
| **핵심 목적**     | 특정 **형식(Style)**, 말투, 도메인 특화 언어 패턴 체득.         | 정확한 **사실(Fact)** 조회, 지식 확장, 근거 기반 답변.                      |
| **지식 업데이트** | 데이터 변경 시 **모델 전체 재학습** 필요 (고비용, 장시간 소요). | 벡터 DB에 **데이터 추가/삭제**만 하면 즉시 반영 (저비용, 실시간).           |
| **환각 제어**     | 학습 데이터에 편향되거나 거짓 정보를 생성할 가능성이 있음.      | 검색된 팩트(Fact) 내에서만 답하게 하여 **환각을 최소화**함.                 |
| **비용 (Cost)**   | 고성능 GPU 및 학습 시간이 필수적임.                             | 초기 구축 비용 외 유지 비용이 저렴하며, 경량 모델로도 높은 성능 가능.       |
| **의료 적용점**   | 퇴원 요약지(BHC/DI) **작성 자동화** (Generation Form).          | 가이드라인 기반 **질의응답**, 유사 환자 **사례 검색** (Evidence Retrieval). |

> **Hybrid Approach (Fine-tuning + RAG)**
> 의료 분야 비즈니스에서는 두 기술을 결합했을 때 가장 강력한 성능을 발휘한다.
> * **Role of Fine-tuning:** 본 연구에서 개발한 모델(TM)을 사용하여 의사가 선호하는 **전문적인 어조와 표준화된 문서 양식**을 생성하는 '두뇌(Brain)' 역할을 수행한다.
> * **Role of RAG:** 최신 의학 논문이나 환자의 개별 EMR 기록을 실시간으로 참조하는 '기억(Memory)' 역할을 수행하여, **구체적인 수치와 사실 관계의 정확성**을 보장한다.

---
